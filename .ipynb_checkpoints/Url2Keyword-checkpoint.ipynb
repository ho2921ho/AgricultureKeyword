{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fed57d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\nh\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n",
      "Requirement already satisfied: selenium in c:\\users\\nh\\anaconda3\\lib\\site-packages (4.7.2)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from selenium) (1.26.11)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from selenium) (2022.9.14)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\nh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\nh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\nh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\nh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\nh\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: keybert in c:\\users\\nh\\anaconda3\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from keybert) (1.21.5)\n",
      "Requirement already satisfied: sentence-transformers>=0.3.8 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from keybert) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from keybert) (1.0.2)\n",
      "Requirement already satisfied: rich>=10.4.0 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from keybert) (13.3.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.14.0 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from rich>=10.4.0->keybert) (2.14.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.1.0 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from rich>=10.4.0->keybert) (2.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.9.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.1.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (1.13.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\nh\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (0.1.97)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (4.26.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\nh\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (3.7)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (0.12.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nh\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (4.64.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\nh\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (0.14.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\nh\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (4.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\nh\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.28.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.1.0->rich>=10.4.0->keybert) (0.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\nh\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers>=0.3.8->keybert) (0.4.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (0.13.2)\n",
      "Requirement already satisfied: click in c:\\users\\nh\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers>=0.3.8->keybert) (8.0.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (9.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.3)\n",
      "Requirement already satisfied: newspaper3k in c:\\users\\nh\\anaconda3\\lib\\site-packages (0.2.8)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from newspaper3k) (3.2.0)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from newspaper3k) (0.35.1)\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from newspaper3k) (4.9.1)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from newspaper3k) (4.11.1)\n",
      "Requirement already satisfied: nltk>=3.2.1 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from newspaper3k) (3.7)\n",
      "Requirement already satisfied: PyYAML>=3.11 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from newspaper3k) (6.0)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from newspaper3k) (0.3)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from newspaper3k) (0.0.4)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from newspaper3k) (1.1.0)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from newspaper3k) (9.2.0)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from newspaper3k) (6.0.10)\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from newspaper3k) (2.28.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from newspaper3k) (2.8.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.3.1)\n",
      "Requirement already satisfied: six in c:\\users\\nh\\anaconda3\\lib\\site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\nh\\anaconda3\\lib\\site-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\nh\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nh\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (2022.7.9)\n",
      "Requirement already satisfied: click in c:\\users\\nh\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (8.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2.0.4)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (3.6.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\nh\\anaconda3\\lib\\site-packages (from click->nltk>=3.2.1->newspaper3k) (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install beautifulsoup4\n",
    "!pip install selenium\n",
    "!pip3 install keybert\n",
    "!pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27ad1395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy\n",
    "import random\n",
    "import pickle\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "import os\n",
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "590e564b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NH\\AppData\\Local\\Temp\\ipykernel_11204\\3290017851.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome (executable_path=r\"C:\\Users\\Notebiz003\\Documents\\chromedriver_win32\\chromedriver.exe\")\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome (executable_path=r\"C:\\Users\\Notebiz003\\Documents\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "416b43a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from_date = '2023-1-30'\n",
    "to_date = '2023-1-31'\n",
    "searching_word = 'metaverse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "808c7b2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████▌                                         | 1/2 [01:41<01:41, 101.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/30/2023  완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [03:20<00:00, 100.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/31/2023  완료\n",
      "1/30/2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:04,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 3건 중 3 건 크롤링 성공\n",
      "1-30-2023  완료\n",
      "1/31/2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:09,  3.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 3건 중 3 건 크롤링 성공\n",
      "1-31-2023  완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:12<00:00,  6.24s/it]\n"
     ]
    }
   ],
   "source": [
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print ('Error: Creating directory. ' +  directory)\n",
    "        \n",
    "cwd = os.getcwd()\n",
    "main_path = cwd + '/' + searching_word\n",
    "\n",
    "createFolder(main_path)\n",
    "createFolder(main_path + '/' + 'url')\n",
    "createFolder(main_path + '/' + 'news_backup')\n",
    "createFolder(main_path + '/' + 'news')\n",
    "createFolder(main_path + '/' + 'keyword')\n",
    "\n",
    "# 1 키워드 검색 뉴스 url 수집\n",
    "def page2url(searching_word,date):\n",
    "    urls = []\n",
    "    for start in range(0, 360, 10):\n",
    "        main_url = 'https://www.google.co.kr/search?q={}&tbs=cdr:1,cd_min:{},cd_max:{}&tbm=nws&ei=dPP-Yu_eCJLL-Qb55bvQDA&start={}&sa=N&ved=2ahUKEwjv6Lvy69H5AhWSZd4KHfnyDso4ChDy0wN6BAgBEDk&biw=1536&bih=754&dpr=1.25'.format(searching_word,date,date,start)\n",
    "        driver.get(url=main_url)\n",
    "        elements = driver.find_elements(By.TAG_NAME, 'a')\n",
    "        lnks = []\n",
    "        for lnk in elements:\n",
    "            lnk = str(lnk.get_attribute('href'))\n",
    "            if 'google' not in lnk and lnk != 'None':\n",
    "                lnks.append(lnk)\n",
    "        if len(lnks) == 0:\n",
    "            print(date, ' Url 수집 완료')\n",
    "            break\n",
    "        urls.extend(lnks)\n",
    "        rand_value =random.uniform(2, 4)\n",
    "        time.sleep(rand_value)\n",
    "        \n",
    "    return urls\n",
    "\n",
    "datelist = pd.date_range(start=from_date, end=to_date).tolist()\n",
    "dtlst = []\n",
    "for d_t in datelist:\n",
    "    d_t = str(d_t)[0:-9]\n",
    "    d = datetime.strptime(d_t, '%Y-%m-%d')\n",
    "    d = d.strftime('%m/%d/%Y')\n",
    "    d = d[0].replace('0','') + d[1:]\n",
    "    d = d[:-7] + d[-7].replace('0','') + d[-6:]\n",
    "    dtlst.append(d)\n",
    "    \n",
    "    \n",
    "Urls = dict()\n",
    "for date in tqdm(dtlst):\n",
    "    urls = page2url(searching_word,date)\n",
    "    Urls[date] = urls\n",
    "    \n",
    "name = \"Urls{}-{}.pickle\".format(from_date,to_date)\n",
    "name = name.replace('/','.')\n",
    "\n",
    "with open(main_path + '/' + 'url/' + name,'wb') as f:\n",
    "    pickle.dump(Urls,f)\n",
    "\n",
    "# 2  수집된 url로 news 크롤링 \n",
    "with open(main_path + '/' + 'url/' + name, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# start = 0\n",
    "for date in list(data):\n",
    "    cnt = 0\n",
    "    tmp = []\n",
    "    for idx,url in tqdm(enumerate(data[date])):\n",
    "        try:\n",
    "            article = Article(url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            v = article.text\n",
    "            tmp.append(v)\n",
    "            cnt += 1\n",
    "        except:\n",
    "            pass\n",
    "            # print(url)\n",
    "    data[date] = [tmp]\n",
    "    print(\"{}에 총 {}건 중 {} 건 크롤링 완료\".format(date,idx+1,cnt))\n",
    "    date = date.replace('/','-')\n",
    "    with open(main_path + '/' + 'news_backup/' + 'news-' + date + '.pickle','wb') as f:\n",
    "        pickle.dump(data,f)\n",
    "    \n",
    "data = pd.DataFrame(data).T\n",
    "\n",
    "data = data.rename({0:\"news\"},axis= 1)\n",
    "\n",
    "name = \"News{}-{}.pickle\".format(from_date,to_date)\n",
    "name = name.replace('/','.')\n",
    "\n",
    "with open(main_path + '/' + 'news/' + name,'wb') as f:\n",
    "    pickle.dump(data,f)\n",
    "    \n",
    "# 3 news에서 키워드를 추출 (keybert)\n",
    "\n",
    "def doc2key(cleaned_content):\n",
    "    \n",
    "    kw_model = KeyBERT(model='all-MiniLM-L6-v2')\n",
    "    keywords = kw_model.extract_keywords(doc)\n",
    "    \n",
    "    n2_kwd = kw_model.extract_keywords(cleaned_content, keyphrase_ngram_range=(2, 2), stop_words='english',\n",
    "                                  use_mmr=True, diversity=0.7, top_n=5)\n",
    "\n",
    "    n1_kwd = kw_model.extract_keywords(cleaned_content, keyphrase_ngram_range=(1, 1), stop_words='english',\n",
    "                                  use_mmr=True, diversity=0.7, top_n=10)\n",
    "    for idx,i in enumerate(n2_kwd):\n",
    "        n2_kwd[idx] = i[0]\n",
    "    for idx,i in enumerate(n1_kwd):\n",
    "        n1_kwd[idx] = i[0]  \n",
    "\n",
    "    n1_kwd.extend(n2_kwd)\n",
    "    kwd = n1_kwd\n",
    "    return kwd\n",
    "\n",
    "with open(main_path + '/' + 'news/' + name, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "data = data.reset_index()\n",
    "data = data.rename({\"index\":\"date\"},axis = 1)\n",
    "\n",
    "idx = []\n",
    "for x in data['date']:\n",
    "    tmp = x.split('/')\n",
    "    if len(tmp[0]) == 1:\n",
    "        tmp[0] = '0'+tmp[0]\n",
    "    if len(tmp[1]) == 1:\n",
    "        tmp[1] = '0'+tmp[1]\n",
    "    tmp = tmp[2] + tmp[0] + tmp[1]\n",
    "    idx.append(''.join(tmp))\n",
    "    \n",
    "data['date'] = idx\n",
    "data = data.rename({'date':'일자'},axis = 1)\n",
    "\n",
    "kwd_list = []\n",
    "for docs in tqdm(data['news']):\n",
    "    kwds = []\n",
    "    for doc in docs:\n",
    "        try:\n",
    "            cleaned_content = re.sub(r'[^\\.\\?\\!\\w\\d\\s]','',doc) # 문장단위로 끊기\n",
    "            cleaned_content = cleaned_content.replace('\\n',' ')\n",
    "            cleaned_content = cleaned_content.lower()\n",
    "            kwd = doc2key(cleaned_content)\n",
    "            kwds.append(kwd)\n",
    "            print(\"Keyword 추출 중...\")\n",
    "        except:\n",
    "            print(doc)\n",
    "    kwd_list.append(kwds)\n",
    "    print(\"Keyword 추출 완료\")\n",
    "    \n",
    "data['kwd'] = kwd_list\n",
    "\n",
    "name = \"Keyword{}-{}.pickle\".format(from_date,to_date)\n",
    "name = name.replace('/','.')\n",
    "\n",
    "with open(main_path + '/' + 'keyword/' + name,'wb') as f:\n",
    "    pickle.dump(data,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c0a8c7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>일자</th>\n",
       "      <th>news</th>\n",
       "      <th>kwd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20230130</td>\n",
       "      <td>[Floki’s price is rallying in a seven-day run ...</td>\n",
       "      <td>[[tokens, floki, trillion, bridge, calamitous,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20230131</td>\n",
       "      <td>[El mercado global Juegos de metaverso exhibe ...</td>\n",
       "      <td>[[información, salesmarketintelligencedata, co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         일자                                               news  \\\n",
       "0  20230130  [Floki’s price is rallying in a seven-day run ...   \n",
       "1  20230131  [El mercado global Juegos de metaverso exhibe ...   \n",
       "\n",
       "                                                 kwd  \n",
       "0  [[tokens, floki, trillion, bridge, calamitous,...  \n",
       "1  [[información, salesmarketintelligencedata, co...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. 워드클라우드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a633fc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 연관도 분석"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
