{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "02839b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromedriver_autoinstaller\n",
      "  Downloading chromedriver_autoinstaller-0.4.0-py3-none-any.whl (6.5 kB)\n",
      "Installing collected packages: chromedriver_autoinstaller\n",
      "Successfully installed chromedriver_autoinstaller-0.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install chromedriver_autoinstaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf5509b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\nh\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n",
      "Requirement already satisfied: selenium in c:\\users\\nh\\anaconda3\\lib\\site-packages (4.7.2)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from selenium) (1.26.11)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from selenium) (2022.9.14)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: outcome in c:\\users\\nh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: idna in c:\\users\\nh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\nh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\nh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\nh\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: newspaper3k in c:\\users\\nh\\anaconda3\\lib\\site-packages (0.2.8)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from newspaper3k) (9.2.0)\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from newspaper3k) (4.9.1)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from newspaper3k) (4.11.1)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from newspaper3k) (3.2.0)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from newspaper3k) (0.0.4)\n",
      "Requirement already satisfied: PyYAML>=3.11 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from newspaper3k) (6.0)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from newspaper3k) (6.0.10)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from newspaper3k) (2.8.2)\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from newspaper3k) (2.28.1)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from newspaper3k) (0.35.1)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from newspaper3k) (1.1.0)\n",
      "Requirement already satisfied: nltk>=3.2.1 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from newspaper3k) (3.7)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from newspaper3k) (0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.3.1)\n",
      "Requirement already satisfied: six in c:\\users\\nh\\anaconda3\\lib\\site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\nh\\anaconda3\\lib\\site-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nh\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\nh\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (2022.7.9)\n",
      "Requirement already satisfied: click in c:\\users\\nh\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (8.0.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (1.26.11)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\nh\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (3.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nh\\anaconda3\\lib\\site-packages (from click->nltk>=3.2.1->newspaper3k) (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install beautifulsoup4\n",
    "!pip install selenium\n",
    "!pip install newspaper3k\n",
    "!pip3 install keybert\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "764b4390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy\n",
    "import random\n",
    "import pickle\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c45d7d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NH\\AppData\\Local\\Temp\\ipykernel_2580\\1447849451.py:19: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(f'./{chrome_ver}/chromedriver.exe', options=option)\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "import chromedriver_autoinstaller\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(r\"c:\\chrometemp\")  #쿠키 / 캐쉬파일 삭제\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "subprocess.Popen(r'C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe --remote-debugging-port=9222 --user-data-dir=\"C:\\chrometemp\"') # 디버거 크롬 구동\n",
    "\n",
    "\n",
    "option = Options()\n",
    "option.add_experimental_option(\"debuggerAddress\", \"127.0.0.1:9222\")\n",
    "\n",
    "chrome_ver = chromedriver_autoinstaller.get_chrome_version().split('.')[0]\n",
    "try:\n",
    "    driver = webdriver.Chrome(f'./{chrome_ver}/chromedriver.exe', options=option)\n",
    "except:\n",
    "    chromedriver_autoinstaller.install(True)\n",
    "    driver = webdriver.Chrome(f'./{chrome_ver}/chromedriver.exe', options=option)\n",
    "driver.implicitly_wait(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2859913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from_date = '20230313'\n",
    "to_date = '20230313'\n",
    "searching_word = 'economy'\n",
    "cwd = os.getcwd()\n",
    "main_path = cwd + '/' + searching_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a3eb69e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/13/2023 Url 수집 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [02:44<00:00, 164.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/13/2023  Url 수집 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print ('Error: Creating directory. ' +  directory)\n",
    "\n",
    "createFolder(main_path)\n",
    "createFolder(main_path + '/' + 'url')\n",
    "createFolder(main_path + '/' + 'news_backup')\n",
    "createFolder(main_path + '/' + 'news')\n",
    "createFolder(main_path + '/' + 'keyword')\n",
    "\n",
    "# 1 키워드 검색 뉴스 url 수집\n",
    "def page2url(searching_word,date):\n",
    "    searching_word = '\"{}\"'.format(searching_word)\n",
    "    urls = []\n",
    "    for start in range(0, 360, 10): ### 하루에 관련 기사를 최대 얼마나 뽑을지 설정.\n",
    "        main_url = 'https://www.google.co.kr/search?q={}&tbs=cdr:1,cd_min:{},cd_max:{}&tbm=nws&ei=dPP-Yu_eCJLL-Qb55bvQDA&start={}&sa=N&ved=2ahUKEwjv6Lvy69H5AhWSZd4KHfnyDso4ChDy0wN6BAgBEDk&biw=1536&bih=754&dpr=1.25'.format(searching_word,date,date,start)\n",
    "        driver.get(url=main_url)\n",
    "        elements = driver.find_elements(By.TAG_NAME, 'a')\n",
    "        lnks = []\n",
    "        for lnk in elements:\n",
    "            lnk = str(lnk.get_attribute('href'))\n",
    "            if 'google' not in lnk and lnk != 'None':\n",
    "                lnks.append(lnk)\n",
    "        if len(lnks) == 0:\n",
    "            print(date, ' Url 수집 완료')\n",
    "            break\n",
    "        urls.extend(lnks)\n",
    "        rand_value =random.uniform(4, 10)\n",
    "        time.sleep(rand_value)\n",
    "        \n",
    "    return urls\n",
    "\n",
    "datelist = pd.date_range(start=from_date, end=to_date).tolist()\n",
    "dtlst = []\n",
    "for d_t in datelist:\n",
    "    d_t = str(d_t)[0:-9]\n",
    "    d = datetime.strptime(d_t, '%Y-%m-%d')\n",
    "    d = d.strftime('%m/%d/%Y')\n",
    "    d = d[0].replace('0','') + d[1:]\n",
    "    d = d[:-7] + d[-7].replace('0','') + d[-6:]\n",
    "    dtlst.append(d)\n",
    "    \n",
    "    \n",
    "Urls = dict()\n",
    "for date in tqdm(dtlst):\n",
    "    print(date,'Url 수집 시작')\n",
    "    urls = page2url(searching_word,date)\n",
    "    Urls[date] = urls\n",
    "    \n",
    "name = \"Urls{}-{}.pickle\".format(from_date,to_date)\n",
    "name = name.replace('/','.')\n",
    "\n",
    "with open(main_path + '/' + 'url/' + name,'wb') as f:\n",
    "    pickle.dump(Urls,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e592dd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/13/2023 크롤링 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [00:49,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.esi-africa.com/multimedia/exclusive-alan-winde-details-western-capes-plans-for-the-green-economy/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [01:01,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.forbes.com/sites/forbescoachescouncil/2023/03/13/why-economic-news-affects-your-personal-finances-and-how-to-get-informed/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46it [01:07,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://telecoms.com/520569/sky-blames-economy-for-1200-italy-job-cuts/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [01:28,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.pymnts.com/technology/2023/barriers-to-a-fully-connected-economy-include-the-fight-for-utility-pole-rights/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "97it [02:21,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://leaderpost.com/sponsored/top-employers-rlp/supporting-saskatchewans-robust-economy-requires-robust-recruitment-strategies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:23,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.moneyweb.co.za/in-depth/revix/how-cryptocurrencies-can-help-protect-south-africans-in-a-weakening-economy/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "103it [02:27,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.moneyweb.co.za/investing/sa-economy-battered-and-bruised-but-not-broken/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "106it [02:30,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.barrons.com/articles/svb-collapse-not-significant-risk-to-europe-eu-economy-chief-a470a3fa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [02:35,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://theberkshireedge.com/connections-can-we-make-predictions-on-the-economy-based-on-womens-fashion/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "115it [02:39,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.miragenews.com/supporting-sustainable-blue-economy-development-965179/\n",
      "https://www.miragenews.com/fednor-invests-2-2m-in-muskoka-for-jobs-economy-965358/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "116it [02:40,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.miragenews.com/vietnams-economy-forecast-to-grow-6-3-in-2023-965910/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "122it [02:57,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.miamiherald.com/news/nation-world/national/article273074900.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "166it [04:23,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.miamiherald.com/opinion/op-ed/article272954980.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "172it [04:31,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.fastcompany.com/90865317/uber-lyft-prop-22-victory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "173it [04:33,  1.67s/it]C:\\Users\\NH\\anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname EDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "179it [04:39,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.torfx.com/post/2023-03-13_gbpusd-weekly-gbp-rallies-on-gdp-growth-federal-reserve-bank-of-england/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "183it [04:44,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.dailyexpress.com.my/news/209273/swepa-marks-women-s-day-with-forum-on-digital-economy/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "198it [05:11,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.dailymirror.lk/financial-news/Sampath-Bank-to-boost-economy-with-IFCs-US-400mn-cross-currency-swap/265-255768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "209it [05:24,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.weeklyblitz.net/news/lenta-ru-economist-tverdokhleb-predicted-irreversible-consequences-of-the-white-houses-policy-for-the-us-economy/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "210it [05:26,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 210건 중 191 건 크롤링 성공\n",
      "3-13-2023  크롤링 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "name = \"Urls{}-{}.pickle\".format(from_date,to_date)\n",
    "name = name.replace('/','.')\n",
    "# 2  수집된 url로 news 크롤링 \n",
    "with open(main_path + '/' + 'url/' + name, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# start = 0\n",
    "for date in list(data):\n",
    "    print(date, '크롤링 시작')\n",
    "    cnt = 0\n",
    "    tmp = []\n",
    "    urls = []\n",
    "    for idx,url in tqdm(enumerate(data[date])):\n",
    "        try:\n",
    "            article = Article(url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            v = article.text\n",
    "            tmp.append(v)\n",
    "            urls.append(url)\n",
    "            cnt += 1\n",
    "        except:\n",
    "            pass\n",
    "            tmp.append(\"null\")\n",
    "            urls.append(url)\n",
    "            print(url)\n",
    "    print(\"총 {}건 중 {} 건 크롤링 성공\".format(idx+1,cnt))\n",
    "    data[date] = [tmp,[urls]]\n",
    "    date = date.replace('/','-')\n",
    "    with open(main_path + '/' + 'news_backup/' + 'news-' + date + '.pickle','wb') as f:\n",
    "        pickle.dump(data,f)\n",
    "    print(date,' 크롤링 완료')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c1fd696",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data).T\n",
    "\n",
    "data = data.rename({0:\"news\",1:\"url\"},axis= 1)\n",
    "\n",
    "name = \"News{}-{}.pickle\".format(from_date,to_date)\n",
    "name = name.replace('/','.')\n",
    "\n",
    "with open(main_path + '/' + 'news/' + name,'wb') as f:\n",
    "    pickle.dump(data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31275483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "import pickle\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2eefa909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2key(cleaned_content):\n",
    "    \n",
    "    kw_model = KeyBERT(model='all-MiniLM-L6-v2')\n",
    "    keywords = kw_model.extract_keywords(cleaned_content)\n",
    "    \n",
    "    n2_kwd = kw_model.extract_keywords(cleaned_content, keyphrase_ngram_range=(2, 2), stop_words='english',\n",
    "                                  use_mmr=True, diversity=0.7, top_n=5)\n",
    "\n",
    "    n1_kwd = kw_model.extract_keywords(cleaned_content, keyphrase_ngram_range=(1, 1), stop_words='english',\n",
    "                                  use_mmr=True, diversity=0.7, top_n=45)\n",
    "    for idx,i in enumerate(n2_kwd):\n",
    "        n2_kwd[idx] = i[0]\n",
    "    for idx,i in enumerate(n1_kwd):\n",
    "        n1_kwd[idx] = i[0]  \n",
    "\n",
    "    n1_kwd.extend(n2_kwd)\n",
    "    kwd = n1_kwd\n",
    "    return kwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c90fddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeKeywordLsit(data,nn_jj = False):\n",
    "    kwd_list = []\n",
    "    for docs in tqdm(data['news']):\n",
    "        kwds = []\n",
    "        for doc in docs:\n",
    "            try:\n",
    "                cleaned_content = re.sub(r'[^\\.\\?\\!\\w\\d\\s]','',doc) # 문장단위로 끊기\n",
    "                cleaned_content = cleaned_content.replace('\\n',' ')\n",
    "                cleaned_content = cleaned_content.lower()\n",
    "                kwd = doc2key(cleaned_content)\n",
    "                if nn_jj == True:\n",
    "                    tokens_pos = nltk.pos_tag(kwd)\n",
    "                    kwd_nn_jj = []\n",
    "                    for word, pos in tokens_pos:\n",
    "                        if 'NN' in pos or 'JJ' in pos:\n",
    "                            kwd_nn_jj.append(word)\n",
    "                    kwds.append(kwd_nn_jj)\n",
    "                else:\n",
    "                    kwds.append(kwd)\n",
    "            except:\n",
    "                print(doc)\n",
    "        kwd_list.append(kwds)\n",
    "    return kwd_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fea1bf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터불러오기\n",
    "\n",
    "name = \"News{}-{}.pickle\".format(from_date,to_date)\n",
    "name = name.replace('/','.')\n",
    "\n",
    "with open(main_path + '/' + 'news/' + name, 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cae9d68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "data = data.reset_index()\n",
    "data = data.rename({\"index\":\"date\"},axis = 1)\n",
    "\n",
    "idx = []\n",
    "for x in data['date']:\n",
    "    tmp = x.split('/')\n",
    "    if len(tmp[0]) == 1:\n",
    "        tmp[0] = '0'+tmp[0]\n",
    "    if len(tmp[1]) == 1:\n",
    "        tmp[1] = '0'+tmp[1]\n",
    "    tmp = tmp[2] + tmp[0] + tmp[1]\n",
    "    idx.append(''.join(tmp))\n",
    "    \n",
    "data['date'] = idx\n",
    "data = data.rename({'date':'일자'},axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ba50b53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [05:11<00:00, 311.74s/it]\n"
     ]
    }
   ],
   "source": [
    "# 키워드 추출\n",
    "kwd_list = makeKeywordLsit(data, nn_jj = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1df0e020",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 데이터 추가\n",
    "data['키워드'] = kwd_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fcbdf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "name = \"Keyword{}-{}.pickle\".format(from_date,to_date)\n",
    "name = name.replace('/','.')\n",
    "with open(main_path + '/' + 'keyword/' + name,'wb') as f:\n",
    "    pickle.dump(data,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
